{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dep</th>\n",
       "      <th>circ</th>\n",
       "      <th>year</th>\n",
       "      <th>pop</th>\n",
       "      <th>dens</th>\n",
       "      <th>age_1</th>\n",
       "      <th>age_2</th>\n",
       "      <th>age_3</th>\n",
       "      <th>age_4</th>\n",
       "      <th>etud</th>\n",
       "      <th>...</th>\n",
       "      <th>presid_ED</th>\n",
       "      <th>presid_EG</th>\n",
       "      <th>presid_G</th>\n",
       "      <th>presid_autre</th>\n",
       "      <th>l_C</th>\n",
       "      <th>l_D</th>\n",
       "      <th>l_ED</th>\n",
       "      <th>l_EG</th>\n",
       "      <th>l_G</th>\n",
       "      <th>l_autre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1993.0</td>\n",
       "      <td>488244.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.64</td>\n",
       "      <td>...</td>\n",
       "      <td>16.09</td>\n",
       "      <td>12.18</td>\n",
       "      <td>31.41</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>46.47</td>\n",
       "      <td>10.28</td>\n",
       "      <td>19.15</td>\n",
       "      <td>24.11</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1993.0</td>\n",
       "      <td>488244.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.64</td>\n",
       "      <td>...</td>\n",
       "      <td>16.09</td>\n",
       "      <td>12.18</td>\n",
       "      <td>31.41</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>46.53</td>\n",
       "      <td>18.79</td>\n",
       "      <td>20.73</td>\n",
       "      <td>13.95</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1993.0</td>\n",
       "      <td>488244.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.64</td>\n",
       "      <td>...</td>\n",
       "      <td>16.09</td>\n",
       "      <td>12.18</td>\n",
       "      <td>31.41</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.59</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.41</td>\n",
       "      <td>18.83</td>\n",
       "      <td>17.18</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1993.0</td>\n",
       "      <td>488244.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.64</td>\n",
       "      <td>...</td>\n",
       "      <td>16.09</td>\n",
       "      <td>12.18</td>\n",
       "      <td>31.41</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.86</td>\n",
       "      <td>0.00</td>\n",
       "      <td>17.46</td>\n",
       "      <td>17.72</td>\n",
       "      <td>15.95</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1993.0</td>\n",
       "      <td>538219.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.93</td>\n",
       "      <td>...</td>\n",
       "      <td>13.41</td>\n",
       "      <td>16.56</td>\n",
       "      <td>39.55</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>42.65</td>\n",
       "      <td>12.33</td>\n",
       "      <td>21.31</td>\n",
       "      <td>23.70</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  dep  circ    year       pop  dens  age_1  age_2  age_3  age_4  etud  \\\n",
       "0   1   1.0  1993.0  488244.0  85.0   29.0   30.0   24.0   18.0  0.64   \n",
       "1   1   2.0  1993.0  488244.0  85.0   29.0   30.0   24.0   18.0  0.64   \n",
       "2   1   3.0  1993.0  488244.0  85.0   29.0   30.0   24.0   18.0  0.64   \n",
       "3   1   4.0  1993.0  488244.0  85.0   29.0   30.0   24.0   18.0  0.64   \n",
       "4   2   1.0  1993.0  538219.0  73.0   29.0   29.0   22.0   19.0  0.93   \n",
       "\n",
       "    ...     presid_ED  presid_EG  presid_G  presid_autre    l_C    l_D   l_ED  \\\n",
       "0   ...         16.09      12.18     31.41           0.0   0.00  46.47  10.28   \n",
       "1   ...         16.09      12.18     31.41           0.0   0.00  46.53  18.79   \n",
       "2   ...         16.09      12.18     31.41           0.0  51.59   0.00  12.41   \n",
       "3   ...         16.09      12.18     31.41           0.0  48.86   0.00  17.46   \n",
       "4   ...         13.41      16.56     39.55           0.0   0.00  42.65  12.33   \n",
       "\n",
       "    l_EG    l_G  l_autre  \n",
       "0  19.15  24.11      0.0  \n",
       "1  20.73  13.95      0.0  \n",
       "2  18.83  17.18      0.0  \n",
       "3  17.72  15.95      0.0  \n",
       "4  21.31  23.70      0.0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#on testera principalement random forest et logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from __future__ import division\n",
    "from collections import Counter\n",
    "\n",
    "#Importer la frame\n",
    "\n",
    "path = '../data/merge.csv'\n",
    "df = pd.read_csv(path, index_col=0)\n",
    "\n",
    "#Traiter la frame\n",
    "df.dep = df.dep.apply(lambda x: x.replace('2A', '20').replace('2B', '20'))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>...</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>971</th>\n",
       "      <th>972</th>\n",
       "      <th>973</th>\n",
       "      <th>974</th>\n",
       "      <th>976</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1993.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1993.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1993.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1993.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1993.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1  10  11  12  13  14  15  16  17  18   ...    92  93  94  95  971  972  \\\n",
       "0  1   0   0   0   0   0   0   0   0   0   ...     0   0   0   0    0    0   \n",
       "1  1   0   0   0   0   0   0   0   0   0   ...     0   0   0   0    0    0   \n",
       "2  1   0   0   0   0   0   0   0   0   0   ...     0   0   0   0    0    0   \n",
       "3  1   0   0   0   0   0   0   0   0   0   ...     0   0   0   0    0    0   \n",
       "4  0   0   0   0   0   0   0   0   0   0   ...     0   0   0   0    0    0   \n",
       "\n",
       "   973  974  976    year  \n",
       "0    0    0    0  1993.0  \n",
       "1    0    0    0  1993.0  \n",
       "2    0    0    0  1993.0  \n",
       "3    0    0    0  1993.0  \n",
       "4    0    0    0  1993.0  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Stockons dès maintenant une binarisation des départements\n",
    "#garder l'année au cas où\n",
    "\n",
    "ddeps = pd.get_dummies(df.dep).join(df.year)\n",
    "ddeps.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "groups = ['EG', 'G', 'C', 'D', 'ED', 'autre']\n",
    "lgroups = ['l_%s' % g for g in groups]\n",
    "pgroups = ['presid_%s' % g for g in groups]\n",
    "\n",
    "#on stocke le \"gagnant\" de chaque circonscription\n",
    "df['l_win'] = df[lgroups].idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#On test des classifications en utilisant juste les présidentielles\n",
    "X = df[pgroups]\n",
    "y = df.l_win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 0.05565508,  0.03636726,  0.04080391,  0.04184739,  0.04290032,\n",
       "         0.04259205,  0.04279065]),\n",
       " 'mean_score_time': array([ 0.01329557,  0.00205533,  0.00203737,  0.00200868,  0.00204102,\n",
       "         0.00206168,  0.00201035]),\n",
       " 'mean_test_score': array([ 0.44676471,  0.42529412,  0.42647059,  0.425     ,  0.425     ,\n",
       "         0.425     ,  0.425     ]),\n",
       " 'mean_train_score': array([ 0.58124111,  0.58344953,  0.58344888,  0.58344888,  0.58344888,\n",
       "         0.58344888,  0.58344888]),\n",
       " 'param_C': masked_array(data = [0.001 0.01 0.1 1 10 100 1000],\n",
       "              mask = [False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'params': ({'C': 0.001},\n",
       "  {'C': 0.01},\n",
       "  {'C': 0.1},\n",
       "  {'C': 1},\n",
       "  {'C': 10},\n",
       "  {'C': 100},\n",
       "  {'C': 1000}),\n",
       " 'rank_test_score': array([1, 3, 2, 4, 4, 4, 4], dtype=int32),\n",
       " 'split0_test_score': array([ 0.27112676,  0.25616197,  0.25528169,  0.25528169,  0.25528169,\n",
       "         0.25528169,  0.25528169]),\n",
       " 'split0_train_score': array([ 0.68109541,  0.68727915,  0.68639576,  0.68639576,  0.68639576,\n",
       "         0.68639576,  0.68639576]),\n",
       " 'split1_test_score': array([ 0.43071492,  0.37775816,  0.37775816,  0.3733451 ,  0.3733451 ,\n",
       "         0.3733451 ,  0.3733451 ]),\n",
       " 'split1_train_score': array([ 0.5513895 ,  0.55227172,  0.55227172,  0.55227172,  0.55227172,\n",
       "         0.55227172,  0.55227172]),\n",
       " 'split2_test_score': array([ 0.63925729,  0.64279399,  0.64721485,  0.64721485,  0.64721485,\n",
       "         0.64721485,  0.64721485]),\n",
       " 'split2_train_score': array([ 0.51123843,  0.51079771,  0.51167915,  0.51167915,  0.51167915,\n",
       "         0.51167915,  0.51167915]),\n",
       " 'std_fit_time': array([ 0.03165473,  0.0006791 ,  0.00061788,  0.00113378,  0.00135129,\n",
       "         0.00150058,  0.00179408]),\n",
       " 'std_score_time': array([  1.50830176e-02,   6.92083137e-05,   5.13496773e-05,\n",
       "          5.48189938e-06,   2.32742888e-05,   6.03496824e-05,\n",
       "          2.59083960e-05]),\n",
       " 'std_test_score': array([ 0.15072702,  0.16139061,  0.1636811 ,  0.16413135,  0.16413135,\n",
       "         0.16413135,  0.16413135]),\n",
       " 'std_train_score': array([ 0.07248534,  0.07534572,  0.07465692,  0.07465692,  0.07465692,\n",
       "         0.07465692,  0.07465692])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gridsearch sur plusieurs hyperparamètres. Pour régression logistique, on test différentes forces de pénalisation\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000] }\n",
    "\n",
    "clf = GridSearchCV(LogisticRegression(), param_grid)\n",
    "\n",
    "clf.fit(X, y)\n",
    "\n",
    "clf.cv_results_\n",
    "\n",
    "#manifestement c'est la plus forte pénalisation (C=0.001) qui donne le meilleur std test score... 0.44 de précision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Testons en limitant le nombre d'annees puisque l'apport des résultats de 2017 dans le jeu d'entraînement n'améliore pas\n",
    "#les scores de validation\n",
    "\n",
    "X2 = df[df.year >= 2012.0][pgroups]\n",
    "y2 = df[df.year >= 2012.0].l_win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'penalty': 'l1', 'C': 0.01}\n",
      "0.670250896057\n",
      "__\n",
      "{'penalty': 'l2', 'C': 0.001}\n",
      "0.668458781362\n",
      "__\n",
      "{'penalty': 'l2', 'C': 0.005}\n",
      "0.666666666667\n",
      "__\n",
      "{'penalty': 'l2', 'C': 0.01}\n",
      "0.666666666667\n",
      "__\n",
      "{'penalty': 'l2', 'C': 0.0005}\n",
      "0.661290322581\n",
      "__\n",
      "{'penalty': 'l1', 'C': 0.005}\n",
      "0.660394265233\n",
      "__\n",
      "{'penalty': 'l2', 'C': 0.0001}\n",
      "0.655913978495\n",
      "__\n",
      "{'penalty': 'l1', 'C': 0.0005}\n",
      "0.631720430108\n",
      "__\n",
      "{'penalty': 'l1', 'C': 0.001}\n",
      "0.621863799283\n",
      "__\n",
      "{'penalty': 'l1', 'C': 0.0001}\n",
      "0.385304659498\n",
      "__\n"
     ]
    }
   ],
   "source": [
    "#On répète le grid search avec seulement 2012 et 2017 comme données.\n",
    "#Cette fois en testant aussi la pénalisation l1, et avec surtout des petits C puisqu'on dirait une forte pénalisation\n",
    "#est nécessaire.\n",
    "\n",
    "param_grid = {'penalty': ['l1', 'l2'], 'C': [0.0001, 0.0005, 0.001, 0.005, 0.01] }\n",
    "\n",
    "clf = GridSearchCV(LogisticRegression(), param_grid, cv=10)\n",
    "\n",
    "clf.fit(X2, y2)\n",
    "\n",
    "for i in np.argsort(clf.cv_results_['rank_test_score'])[:10]:\n",
    "    print clf.cv_results_['params'][i]\n",
    "    print clf.cv_results_['mean_test_score'][i]\n",
    "    print '__'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En limitant le nombre d'années, on limite les différentes situations politiques, d'où les meilleurs scores. C'est sûrement de l'overfit mais on pensera au moins à supprimer les années 93 et 97 de l'entraînement plus tard. Finalement ces deux années n'ont vraiment rien à voir avec le reste des élections législatives puisqu'elles ne voient pas d'élection présidentielle se dérouler.\n",
    "\n",
    "Partons sur des tests similaires pour Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#RF a l'air d'exiger un min_samples_split un minimum élevé pour éviter l'overfit ?\n",
    "#On test aussi le nombre d'arbres et le critère de séparation.\n",
    "\n",
    "#param_grid = {'n_estimators': [10, 50, 100], 'min_samples_split': [100, 200, 300, 400, 500, 600, 700], 'criterion': ['gini', 'entropy']}\n",
    "#\n",
    "#clf = GridSearchCV(RandomForestClassifier(), param_grid, cv=10)\n",
    "#\n",
    "#clf.fit(X, y)\n",
    "#\n",
    "#for i in np.argsort(clf.cv_results_['rank_test_score'])[:10]:\n",
    "#    print clf.cv_results_['params'][i]\n",
    "#    print clf.cv_results_['mean_test_score'][i]\n",
    "#    print '__'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le gridsearch est long à effectuer pour RF. Les meilleurs scores sont : \n",
    "\n",
    "{'min_samples_split': 600, 'n_estimators': 100, 'criterion': 'gini'}\n",
    "0.562647058824\n",
    "\n",
    "{'min_samples_split': 600, 'n_estimators': 50, 'criterion': 'gini'}\n",
    "0.557352941176\n",
    "\n",
    "{'min_samples_split': 600, 'n_estimators': 10, 'criterion': 'gini'}\n",
    "0.556764705882\n",
    "\n",
    "{'min_samples_split': 300, 'n_estimators': 50, 'criterion': 'gini'}\n",
    "0.556176470588\n",
    "\n",
    "{'min_samples_split': 500, 'n_estimators': 50, 'criterion': 'entropy'}\n",
    "0.555882352941\n",
    "\n",
    "{'min_samples_split': 300, 'n_estimators': 10, 'criterion': 'gini'}\n",
    "0.555882352941\n",
    "\n",
    "{'min_samples_split': 600, 'n_estimators': 100, 'criterion': 'entropy'}\n",
    "0.555588235294\n",
    "\n",
    "{'min_samples_split': 500, 'n_estimators': 10, 'criterion': 'gini'}\n",
    "0.555\n",
    "\n",
    "{'min_samples_split': 500, 'n_estimators': 50, 'criterion': 'gini'}\n",
    "0.554705882353\n",
    "\n",
    "{'min_samples_split': 700, 'n_estimators': 50, 'criterion': 'gini'}\n",
    "0.553235294118\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Retestons avec juste les données 2012 et 2017\n",
    "\n",
    "\n",
    "#param_grid = {'n_estimators': [10, 25, 50, 75, 100], 'min_samples_split': [150, 250, 350, 450], 'criterion': ['gini', 'entropy']}\n",
    "#\n",
    "#clf = GridSearchCV(RandomForestClassifier(), param_grid, cv=10)\n",
    "#\n",
    "#clf.fit(X2, y2)\n",
    "#\n",
    "##10 combinaisons aux meilleurs scores, et leurs scores\n",
    "#\n",
    "#for i in np.argsort(clf.cv_results_['rank_test_score'])[:10]:\n",
    "#    print clf.cv_results_['params'][i]\n",
    "#    print clf.cv_results_['mean_test_score'][i]\n",
    "#    print '__'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour 2012 et 2017 les meilleurs scores sont :\n",
    "{'min_samples_split': 250, 'n_estimators': 10, 'criterion': 'entropy'}\n",
    "0.6729390681\n",
    "\n",
    "{'min_samples_split': 250, 'n_estimators': 25, 'criterion': 'entropy'}\n",
    "0.664874551971\n",
    "\n",
    "{'min_samples_split': 250, 'n_estimators': 25, 'criterion': 'gini'}\n",
    "0.662186379928\n",
    "\n",
    "{'min_samples_split': 250, 'n_estimators': 50, 'criterion': 'entropy'}\n",
    "0.660394265233\n",
    "\n",
    "{'min_samples_split': 150, 'n_estimators': 50, 'criterion': 'entropy'}\n",
    "0.656810035842\n",
    "\n",
    "{'min_samples_split': 250, 'n_estimators': 75, 'criterion': 'gini'}\n",
    "0.656810035842\n",
    "\n",
    "{'min_samples_split': 250, 'n_estimators': 100, 'criterion': 'gini'}\n",
    "0.656810035842\n",
    "\n",
    "{'min_samples_split': 250, 'n_estimators': 75, 'criterion': 'entropy'}\n",
    "0.655913978495\n",
    "\n",
    "{'min_samples_split': 150, 'n_estimators': 100, 'criterion': 'gini'}\n",
    "0.655017921147\n",
    "\n",
    "{'min_samples_split': 250, 'n_estimators': 100, 'criterion': 'entropy'}\n",
    "0.655017921147\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Maintenant on s'intéresse à 2007/2012\n",
    "\n",
    "\n",
    "X1 = df[(df.year == 2007) | (df.year == 2012)][pgroups]\n",
    "y1 = df[(df.year == 2007) | (df.year == 2012)].l_win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'penalty': 'l2', 'C': 0.0005}\n",
      "0.675819309123\n",
      "__\n",
      "{'penalty': 'l2', 'C': 0.005}\n",
      "0.674933569531\n",
      "__\n",
      "{'penalty': 'l2', 'C': 0.01}\n",
      "0.674933569531\n",
      "__\n",
      "{'penalty': 'l1', 'C': 0.005}\n",
      "0.674047829938\n",
      "__\n",
      "{'penalty': 'l2', 'C': 1}\n",
      "0.674047829938\n",
      "__\n",
      "{'penalty': 'l2', 'C': 0.001}\n",
      "0.673162090345\n",
      "__\n",
      "{'penalty': 'l2', 'C': 0.1}\n",
      "0.673162090345\n",
      "__\n",
      "{'penalty': 'l1', 'C': 1}\n",
      "0.673162090345\n",
      "__\n",
      "{'penalty': 'l1', 'C': 0.01}\n",
      "0.669619131975\n",
      "__\n",
      "{'penalty': 'l1', 'C': 0.1}\n",
      "0.668733392383\n",
      "__\n"
     ]
    }
   ],
   "source": [
    "#Pour lr\n",
    "\n",
    "param_grid = {'penalty': ['l1', 'l2'], 'C': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1, 1, 10] }\n",
    "\n",
    "clf = GridSearchCV(LogisticRegression(), param_grid, cv=10)\n",
    "\n",
    "clf.fit(X1, y1)\n",
    "\n",
    "for i in np.argsort(clf.cv_results_['rank_test_score'])[:10]:\n",
    "    print clf.cv_results_['params'][i]\n",
    "    print clf.cv_results_['mean_test_score'][i]\n",
    "    print '__'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Puisque scores meilleurs sur les plus gros min_samples_split, on se concentre dessus\n",
    "\n",
    "\n",
    "#param_grid = {'n_estimators': [10, 25, 50, 75, 100], 'min_samples_split': [450, 550, 650, 750], 'criterion': ['gini', 'entropy']}\n",
    "#\n",
    "#clf = GridSearchCV(RandomForestClassifier(), param_grid, cv=10)\n",
    "#\n",
    "#clf.fit(X1, y1)\n",
    "#\n",
    "##10 combinaisons aux meilleurs scores, et leurs scores\n",
    "#\n",
    "#for i in np.argsort(clf.cv_results_['rank_test_score'])[:10]:\n",
    "#    print clf.cv_results_['params'][i]\n",
    "#    print clf.cv_results_['mean_test_score'][i]\n",
    "#    print '__'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 meilleurs scores :\n",
    "\n",
    "{'min_samples_split': 450, 'n_estimators': 100, 'criterion': 'entropy'}\n",
    "0.651018600531\n",
    "\n",
    "{'min_samples_split': 450, 'n_estimators': 10, 'criterion': 'entropy'}\n",
    "0.647475642161\n",
    "\n",
    "{'min_samples_split': 450, 'n_estimators': 75, 'criterion': 'entropy'}\n",
    "0.646589902569\n",
    "\n",
    "{'min_samples_split': 450, 'n_estimators': 50, 'criterion': 'entropy'}\n",
    "0.645704162976\n",
    "\n",
    "{'min_samples_split': 450, 'n_estimators': 75, 'criterion': 'gini'}\n",
    "0.643932683791"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pour 2007/2012, seul :\n",
      "LR : 0.67642\n",
      "RF : 0.64100\n",
      "Pour 2007/2012, LR, dep réintégrée : 0.67288\n",
      "Pour 2007/2012, RF, dep réintégrée : 0.64457\n",
      "Pour 2007/2012, LR, circ réintégrée : 0.67198\n",
      "Pour 2007/2012, RF, circ réintégrée : 0.65524\n",
      "Pour 2007/2012, LR, year réintégrée : 0.67642\n",
      "Pour 2007/2012, RF, year réintégrée : 0.66244\n",
      "Pour 2007/2012, LR, pop réintégrée : 0.66145\n",
      "Pour 2007/2012, RF, pop réintégrée : 0.65347\n",
      "Pour 2007/2012, LR, dens réintégrée : 0.66752\n",
      "Pour 2007/2012, RF, dens réintégrée : 0.64633\n",
      "Pour 2007/2012, LR, age_1 réintégrée : 0.67372\n",
      "Pour 2007/2012, RF, age_1 réintégrée : 0.64902\n",
      "Pour 2007/2012, LR, age_2 réintégrée : 0.67374\n",
      "Pour 2007/2012, RF, age_2 réintégrée : 0.63652\n",
      "Pour 2007/2012, LR, age_3 réintégrée : 0.67642\n",
      "Pour 2007/2012, RF, age_3 réintégrée : 0.64902\n",
      "Pour 2007/2012, LR, age_4 réintégrée : 0.67022\n",
      "Pour 2007/2012, RF, age_4 réintégrée : 0.65080\n",
      "Pour 2007/2012, LR, etud réintégrée : 0.67110\n",
      "Pour 2007/2012, RF, etud réintégrée : 0.64010\n",
      "Pour 2007/2012, LR, hf réintégrée : 0.67642\n",
      "Pour 2007/2012, RF, hf réintégrée : 0.65344\n",
      "Pour 2007/2012, LR, chom réintégrée : 0.67553\n",
      "Pour 2007/2012, RF, chom réintégrée : 0.64546\n",
      "_______\n",
      "Pour 2012/2017, seul :\n",
      "LR : 0.67001\n",
      "RF : 0.65651\n",
      "Pour 2012/2017, LR, dep réintégrée : 0.66194\n",
      "Pour 2012/2017, RF, dep réintégrée : 0.66195\n",
      "Pour 2012/2017, LR, circ réintégrée : 0.66729\n",
      "Pour 2012/2017, RF, circ réintégrée : 0.66101\n",
      "Pour 2012/2017, LR, year réintégrée : 0.66819\n",
      "Pour 2012/2017, RF, year réintégrée : 0.66112\n",
      "Pour 2012/2017, LR, pop réintégrée : 0.66104\n",
      "Pour 2012/2017, RF, pop réintégrée : 0.65384\n",
      "Pour 2012/2017, LR, dens réintégrée : 0.66197\n",
      "Pour 2012/2017, RF, dens réintégrée : 0.66109\n",
      "Pour 2012/2017, LR, age_1 réintégrée : 0.67182\n",
      "Pour 2012/2017, RF, age_1 réintégrée : 0.65564\n",
      "Pour 2012/2017, LR, age_2 réintégrée : 0.67001\n",
      "Pour 2012/2017, RF, age_2 réintégrée : 0.65931\n",
      "Pour 2012/2017, LR, age_3 réintégrée : 0.67001\n",
      "Pour 2012/2017, RF, age_3 réintégrée : 0.66108\n",
      "Pour 2012/2017, LR, age_4 réintégrée : 0.67001\n",
      "Pour 2012/2017, RF, age_4 réintégrée : 0.65205\n",
      "Pour 2012/2017, LR, etud réintégrée : 0.67001\n",
      "Pour 2012/2017, RF, etud réintégrée : 0.65833\n",
      "Pour 2012/2017, LR, hf réintégrée : 0.66643\n",
      "Pour 2012/2017, RF, hf réintégrée : 0.65741\n",
      "Pour 2012/2017, LR, chom réintégrée : 0.66819\n",
      "Pour 2012/2017, RF, chom réintégrée : 0.66010\n"
     ]
    }
   ],
   "source": [
    "#Travail de réintégration des variables. Une variable supplémentaire\n",
    "\n",
    "#LR1 et RF1 correspondent aux meilleurs hyperparamètres pour s'entraîner sur 2007 et 2012.\n",
    "#LR2 et RF2 pour s'entraîner sur 2012 et 2017.\n",
    "\n",
    "#Manifestement pour RF1 il faut ajouter age_3. Pour RF2 age_4.\n",
    "#Pour LR1 rien à faire.\n",
    "\n",
    "LR1 = LogisticRegression(C=0.0005, penalty='l2')\n",
    "LR2 = LogisticRegression(C=0.01, penalty='l1')\n",
    "\n",
    "RF1 = RandomForestClassifier(min_samples_split=450, n_estimators=100, criterion='entropy')\n",
    "RF2 = RandomForestClassifier(min_samples_split=250, n_estimators=75, criterion='entropy')\n",
    "\n",
    "l1 = {}\n",
    "l2 = {}\n",
    "\n",
    "for col in df.columns[:-12]:\n",
    "    l1[col] = X1.join(df[col], how='inner')\n",
    "    l2[col] = X2.join(df[col], how='inner')\n",
    "\n",
    "print \"Pour 2007/2012, seul :\"\n",
    "print \"LR : %0.5f\" % np.mean(cross_val_score(LR1, X1, y1, cv=10))\n",
    "print \"RF : %0.5f\" % np.mean(cross_val_score(RF1, X1, y1, cv=10))\n",
    "for col in df.columns[:-12]:\n",
    "    print \"Pour 2007/2012, LR, %s réintégrée : %0.5f\" % (col, np.mean(cross_val_score(LR1, l1[col], y1, cv=10)))\n",
    "    print \"Pour 2007/2012, RF, %s réintégrée : %0.5f\" % (col, np.mean(cross_val_score(RF1, l1[col], y1, cv=10)))\n",
    "\n",
    "print '_______'\n",
    "print \"Pour 2012/2017, seul :\"\n",
    "print \"LR : %0.5f\" % np.mean(cross_val_score(LR2, X2, y2, cv=10))\n",
    "print \"RF : %0.5f\" % np.mean(cross_val_score(RF2, X2, y2, cv=10))\n",
    "for col in df.columns[:-12]:\n",
    "    print \"Pour 2012/2017, LR, %s réintégrée : %0.5f\" % (col, np.mean(cross_val_score(LR2, l2[col], y2, cv=10)))\n",
    "    print \"Pour 2012/2017, RF, %s réintégrée : %0.5f\" % (col, np.mean(cross_val_score(RF2, l2[col], y2, cv=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pour 2007/2012, seul : 0.676416\n",
      "Pour 2007/2012, pop, age_3 : 0.680966\n",
      "_______\n",
      "Pour 2012/2017, seul : 0.660228\n"
     ]
    }
   ],
   "source": [
    "#Et maintenant on teste des combinaisons en ajoutant 2 variables à la fois. Un algo à la fois. LR\n",
    "\n",
    "ll1 = {}\n",
    "ll2 = {}\n",
    "\n",
    "for col1 in df.columns[:-12]:\n",
    "    for col2 in df.columns[:-12]:\n",
    "        if (col1 != col2) and ((col2, col1) not in ll1.keys()):\n",
    "            ll1[(col1, col2)] = X1.join(df[col1]).join(df[col2])\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "for col1 in df.columns[:-12]:\n",
    "    for col2 in df.columns[:-12]:\n",
    "        if (col1 != col2) and ((col2, col1) not in ll2.keys()):\n",
    "            ll2[(col1, col2)] = X2.join(df[col1]).join(df[col2])\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "print \"Pour 2007/2012, seul : %0.6f\" % np.mean(cross_val_score(LR1, X1, y1, cv=10))\n",
    "for (col1, col2) in ll1.keys():\n",
    "    score = np.mean(cross_val_score(LR1, ll1[col1, col2], y1, cv=10))\n",
    "    if score > 0.678:\n",
    "        print \"Pour 2007/2012, %s, %s : %0.6f\" % (col1, col2, score)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "print '_______'\n",
    "print \"Pour 2012/2017, seul : %0.6f\" % np.mean(cross_val_score(LR2, X2, y2))\n",
    "for (col1, col2) in ll2.keys():\n",
    "    score = np.mean(cross_val_score(LR2, ll2[col1, col2], y2))\n",
    "    if score > 0.678:\n",
    "        print \"Pour 2012/2017, %s, %s : %0.6f\" % (col1, col2, score)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65472915242652086"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pour RF1, ajouter age_2 chom donne 64.46%\n",
    "#Pour RF2, pop et hf (!) donne 0.6665 . Il y a aussi age3 etud qui donne 0.6655. Bref on remonte très légèrement au\n",
    "#dessus du score de base.\n",
    "\n",
    "\n",
    "#print \"Pour 2007/2012, seul : %0.6f\" % np.mean(cross_val_score(RF1, X1, y1, cv=10))\n",
    "#for (col1, col2) in ll1.keys():\n",
    "#    score = np.mean(cross_val_score(RF1, ll1[col1, col2], y1, cv=10))\n",
    "#    if score > 0.65:\n",
    "#        print \"Pour 2007/2012, %s, %s : %0.6f\" % (col1, col2, score)\n",
    "#    else:\n",
    "#        pass\n",
    "\n",
    "#print '_______'\n",
    "#print \"Pour 2012/2017, seul : %0.6f\" % np.mean(cross_val_score(RF2, X2, y2, cv=10))\n",
    "#for (col1, col2) in ll2.keys():\n",
    "#    score = np.mean(cross_val_score(RF2, ll2[col1, col2], y2, cv=10))\n",
    "#    if score > 0.658:\n",
    "#        print \"Pour 2012/2017, %s, %s : %0.6f\" % (col1, col2, score)\n",
    "#    else:\n",
    "#        pass\n",
    "\n",
    "np.mean(cross_val_score(RF2, X2, y2, cv=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Enfin, on teste la sélection de features avec selectfrommodel.\n",
    "\n",
    "#On met en place des frames qui contiennent toutes les variables de train.\n",
    "xcols = df.columns\n",
    "for lg in lgroups:\n",
    "    xcols.remove(lg)\n",
    "xcols.remove('l_win')\n",
    "Xtot = df[xcols]\n",
    "X1tot = df[(df.year == 2007) | (df.year == 2012)][xcols]\n",
    "X2tot = df[(df.year == 2012) | (df.year == 2017)][xcols]\n",
    "\n",
    "\n",
    "PipeLR1 = Pipeline([('feature_selection', SelectFromModel(LR1)),\n",
    "                   ('clf', LR1)])\n",
    "PipeLR2 = Pipeline([('feature_selection', SelectFromModel(LR2)),\n",
    "                   ('clf', LR2)])\n",
    "\n",
    "PipeRF1 = Pipeline([('feature_selection', SelectFromModel(RF1)),\n",
    "                   ('clf', RF1)])\n",
    "PipeRF2 = Pipeline([('feature_selection', SelectFromModel(RF2)),\n",
    "                   ('clf', RF2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2007/2012, LR : 0.67734\n",
      "2007/2012, RF : 0.64993\n",
      "__\n",
      "2012/2017, LR : 0.66018\n",
      "2012/2017, RF : 0.66556\n"
     ]
    }
   ],
   "source": [
    "print \"2007/2012, LR : %0.5f\" % np.mean(cross_val_score(PipeLR1, X1tot, y1, cv=10))\n",
    "print \"2007/2012, RF : %0.5f\" % np.mean(cross_val_score(PipeRF1, X1tot, y1, cv=10))\n",
    "print '__'\n",
    "print \"2012/2017, LR : %0.5f\" % np.mean(cross_val_score(PipeLR2, X2tot, y2, cv=10))\n",
    "print \"2012/2017, RF : %0.5f\" % np.mean(cross_val_score(PipeRF2, X2tot, y2, cv=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.676415680434\n",
      "0.670005126452\n",
      "0.649049056382\n",
      "0.65833304853\n"
     ]
    }
   ],
   "source": [
    "#Pour mémoire\n",
    "print np.mean(cross_val_score(LR1, X1, y1, cv=10))\n",
    "print np.mean(cross_val_score(LR2, X2, y2, cv=10))\n",
    "print np.mean(cross_val_score(RF1, X1, y1, cv=10))\n",
    "print np.mean(cross_val_score(RF2, X2, y2, cv=10))\n",
    "\n",
    "#Donc les scores n'évoluent pas beaucoup. Peut-être qu'il aurait mieux valu sélectionner les features avec un autre\n",
    "#modèle que celui qu'on utilise pour les prédictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Juste histoire de voir les variables choisies par les sfm :\n",
    "\n",
    "sfm_LR1 = SelectFromModel(LR1)\n",
    "sfm_LR2 = SelectFromModel(LR2)\n",
    "\n",
    "sfm_RF1 = SelectFromModel(RF1)\n",
    "sfm_RF2 = SelectFromModel(RF2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sfm_LR1.fit(X1tot, y1)\n",
    "Xsfm_LR1 = sfm_LR1.transform(X1tot)\n",
    "\n",
    "sfm_LR2.fit(X2tot, y2)\n",
    "Xsfm_LR2 = sfm_LR2.transform(X2tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 19.  ,  19.62,  34.5 ,  15.97,   9.24,  20.67],\n",
       "       [ 19.  ,  19.62,  34.5 ,  15.97,   9.24,  20.67],\n",
       "       [ 19.  ,  19.62,  34.5 ,  15.97,   9.24,  20.67],\n",
       "       ..., \n",
       "       [ 20.  ,  11.24,  34.24,  14.17,  12.76,  27.32],\n",
       "       [ 20.  ,  11.24,  34.24,  14.17,  12.76,  27.32],\n",
       "       [ 20.  ,  11.24,  34.24,  14.17,  12.76,  27.32]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ici on a juste exclu presid_autre et intégré age_4\n",
    "Xsfm_LR1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.00000000e+00,   2.01200000e+03,   1.06000000e+02, ...,\n",
       "          2.29500000e+01,   1.34600000e+01,   2.27500000e+01],\n",
       "       [  1.00000000e+00,   2.01200000e+03,   1.06000000e+02, ...,\n",
       "          2.29500000e+01,   1.34600000e+01,   2.27500000e+01],\n",
       "       [  1.00000000e+00,   2.01200000e+03,   1.06000000e+02, ...,\n",
       "          2.29500000e+01,   1.34600000e+01,   2.27500000e+01],\n",
       "       ..., \n",
       "       [  9.74000000e+02,   2.01700000e+03,   3.39000000e+02, ...,\n",
       "          2.63500000e+01,   2.72600000e+01,   7.67000000e+00],\n",
       "       [  9.76000000e+02,   2.01700000e+03,   6.25000000e+02, ...,\n",
       "          3.02700000e+01,   1.13000000e+01,   4.33000000e+00],\n",
       "       [  9.76000000e+02,   2.01700000e+03,   6.25000000e+02, ...,\n",
       "          3.02700000e+01,   1.13000000e+01,   4.33000000e+00]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ici par contre on est sur du dep, year, dens, ED, EG, G\n",
    "Xsfm_LR2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sfm_RF1.fit(X1tot, y1)\n",
    "Xsfm_RF1 = sfm_RF1.transform(X1tot)\n",
    "\n",
    "sfm_RF2.fit(X2tot, y2)\n",
    "Xsfm_RF2 = sfm_RF2.transform(X2tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.00700000e+03,   1.96200000e+01,   3.45000000e+01,\n",
       "          9.24000000e+00,   2.06700000e+01,   0.00000000e+00],\n",
       "       [  2.00700000e+03,   1.96200000e+01,   3.45000000e+01,\n",
       "          9.24000000e+00,   2.06700000e+01,   0.00000000e+00],\n",
       "       [  2.00700000e+03,   1.96200000e+01,   3.45000000e+01,\n",
       "          9.24000000e+00,   2.06700000e+01,   0.00000000e+00],\n",
       "       ..., \n",
       "       [  2.01200000e+03,   1.12400000e+01,   3.42400000e+01,\n",
       "          1.27600000e+01,   2.73200000e+01,   2.80000000e-01],\n",
       "       [  2.01200000e+03,   1.12400000e+01,   3.42400000e+01,\n",
       "          1.27600000e+01,   2.73200000e+01,   2.80000000e-01],\n",
       "       [  2.01200000e+03,   1.12400000e+01,   3.42400000e+01,\n",
       "          1.27600000e+01,   2.73200000e+01,   2.80000000e-01]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#year, C, D, EG, G, autre\n",
    "Xsfm_RF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.01200000e+03,   1.01600000e+01,   3.04100000e+01,\n",
       "          1.34600000e+01,   2.27500000e+01,   2.70000000e-01],\n",
       "       [  2.01200000e+03,   1.01600000e+01,   3.04100000e+01,\n",
       "          1.34600000e+01,   2.27500000e+01,   2.70000000e-01],\n",
       "       [  2.01200000e+03,   1.01600000e+01,   3.04100000e+01,\n",
       "          1.34600000e+01,   2.27500000e+01,   2.70000000e-01],\n",
       "       ..., \n",
       "       [  2.01700000e+03,   1.89100000e+01,   1.72600000e+01,\n",
       "          2.72600000e+01,   7.67000000e+00,   2.54000000e+00],\n",
       "       [  2.01700000e+03,   1.92100000e+01,   3.26200000e+01,\n",
       "          1.13000000e+01,   4.33000000e+00,   2.27000000e+00],\n",
       "       [  2.01700000e+03,   1.92100000e+01,   3.26200000e+01,\n",
       "          1.13000000e+01,   4.33000000e+00,   2.27000000e+00]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#year, C, D, EG, G, autre\n",
    "Xsfm_RF2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La sélection automatique de features n'augmente pas beaucoup les scores de validation. Mais elle nous indique qu'on pourrait supprimer certains des scores de présidentielles des variables d'entraînement, puisqu'ils sont dépendants les uns des autres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#quelle prediction de 2017 pour LR1 et RF1 ?\n",
    "\n",
    "LR1.fit(X1, y1)\n",
    "RF1.fit(X1, y1)\n",
    "\n",
    "test = df[df.year == 2017][pgroups]\n",
    "\n",
    "p_lr1 = LR1.predict(test)\n",
    "p_rf1 = RF1.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'l_D': 57, 'l_G': 501})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(p_lr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'l_D': 239, 'l_G': 319})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(p_rf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'l_D': 171, 'l_G': 387})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pas terrible comme prédiction, surtout quand on voit ce qu'il est advenu des candidats PS cette année.\n",
    "#Mieux avec quelques variables réintégrées ? J'en doute...\n",
    "\n",
    "#Pour LR1 le meilleur score obtenu était 68% en réintégrant pop et age_3\n",
    "X1b = X1.join(df[['pop', 'age_3']])\n",
    "LR1.fit(X1b, y1)\n",
    "\n",
    "test2 = test.join(df[['pop', 'age_3']])\n",
    "\n",
    "p_lr1 = LR1.predict(test2)\n",
    "Counter(p_lr1)\n",
    "#Pas beaucoup mieux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'l_D': 249, 'l_G': 309})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pour RF1 le meilleur score obtenu était 66.2% en réintégrant year... Peu probable que l'année ait une valeur\n",
    "#explicative et le modèle va certainement s'en servir pour faire le tri entre l'année 2007 et l'année 2012.\n",
    "X1c = X1.join(df['year'])\n",
    "RF1.fit(X1c, y1)\n",
    "\n",
    "test3 = test.join(df['year'])\n",
    "\n",
    "p_rf1 = RF1.predict(test3)\n",
    "Counter(p_rf1)\n",
    "#On est toujours sur une prédiction binaire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Le score de précision semble être un indicateur insuffisant de la pertinence des modèles. Même en jouant avec les variables d'entrée, les années utilisées pour l'entraînement et les hyperparamètres pour améliorer le score, on reste sur une prédiction qui donne l'avantage à la gauche et à la droite. Normal, c'est historiquement les deux groupes qui ont \"remporté\" le plus de circonscriptions. Peut-être qu'on devrait faire quelque chose pour que l'algorithme ne donne pas un avantage si prononcé à ces deux groupes ? On pourrait par exemple cloner des lignes des autres classes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
