{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dep</th>\n",
       "      <th>circ</th>\n",
       "      <th>year</th>\n",
       "      <th>pop</th>\n",
       "      <th>dens</th>\n",
       "      <th>age_1</th>\n",
       "      <th>age_2</th>\n",
       "      <th>age_3</th>\n",
       "      <th>age_4</th>\n",
       "      <th>etud</th>\n",
       "      <th>...</th>\n",
       "      <th>presid_ED</th>\n",
       "      <th>presid_EG</th>\n",
       "      <th>presid_G</th>\n",
       "      <th>presid_autre</th>\n",
       "      <th>l_C</th>\n",
       "      <th>l_D</th>\n",
       "      <th>l_ED</th>\n",
       "      <th>l_EG</th>\n",
       "      <th>l_G</th>\n",
       "      <th>l_autre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1993.0</td>\n",
       "      <td>488244.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.64</td>\n",
       "      <td>...</td>\n",
       "      <td>16.09</td>\n",
       "      <td>12.18</td>\n",
       "      <td>31.41</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>46.47</td>\n",
       "      <td>10.28</td>\n",
       "      <td>19.15</td>\n",
       "      <td>24.11</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1993.0</td>\n",
       "      <td>488244.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.64</td>\n",
       "      <td>...</td>\n",
       "      <td>16.09</td>\n",
       "      <td>12.18</td>\n",
       "      <td>31.41</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>46.53</td>\n",
       "      <td>18.79</td>\n",
       "      <td>20.73</td>\n",
       "      <td>13.95</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1993.0</td>\n",
       "      <td>488244.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.64</td>\n",
       "      <td>...</td>\n",
       "      <td>16.09</td>\n",
       "      <td>12.18</td>\n",
       "      <td>31.41</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.59</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.41</td>\n",
       "      <td>18.83</td>\n",
       "      <td>17.18</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1993.0</td>\n",
       "      <td>488244.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.64</td>\n",
       "      <td>...</td>\n",
       "      <td>16.09</td>\n",
       "      <td>12.18</td>\n",
       "      <td>31.41</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.86</td>\n",
       "      <td>0.00</td>\n",
       "      <td>17.46</td>\n",
       "      <td>17.72</td>\n",
       "      <td>15.95</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1993.0</td>\n",
       "      <td>538219.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.93</td>\n",
       "      <td>...</td>\n",
       "      <td>13.41</td>\n",
       "      <td>16.56</td>\n",
       "      <td>39.55</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>42.65</td>\n",
       "      <td>12.33</td>\n",
       "      <td>21.31</td>\n",
       "      <td>23.70</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  dep  circ    year       pop  dens  age_1  age_2  age_3  age_4  etud  \\\n",
       "0   1   1.0  1993.0  488244.0  85.0   29.0   30.0   24.0   18.0  0.64   \n",
       "1   1   2.0  1993.0  488244.0  85.0   29.0   30.0   24.0   18.0  0.64   \n",
       "2   1   3.0  1993.0  488244.0  85.0   29.0   30.0   24.0   18.0  0.64   \n",
       "3   1   4.0  1993.0  488244.0  85.0   29.0   30.0   24.0   18.0  0.64   \n",
       "4   2   1.0  1993.0  538219.0  73.0   29.0   29.0   22.0   19.0  0.93   \n",
       "\n",
       "    ...     presid_ED  presid_EG  presid_G  presid_autre    l_C    l_D   l_ED  \\\n",
       "0   ...         16.09      12.18     31.41           0.0   0.00  46.47  10.28   \n",
       "1   ...         16.09      12.18     31.41           0.0   0.00  46.53  18.79   \n",
       "2   ...         16.09      12.18     31.41           0.0  51.59   0.00  12.41   \n",
       "3   ...         16.09      12.18     31.41           0.0  48.86   0.00  17.46   \n",
       "4   ...         13.41      16.56     39.55           0.0   0.00  42.65  12.33   \n",
       "\n",
       "    l_EG    l_G  l_autre  \n",
       "0  19.15  24.11      0.0  \n",
       "1  20.73  13.95      0.0  \n",
       "2  18.83  17.18      0.0  \n",
       "3  17.72  15.95      0.0  \n",
       "4  21.31  23.70      0.0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#on testera principalement random forest et logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from __future__ import division\n",
    "from collections import Counter\n",
    "\n",
    "#Importer la frame\n",
    "\n",
    "path = '../data/merge.csv'\n",
    "df = pd.read_csv(path, index_col=0)\n",
    "\n",
    "#Traiter la frame\n",
    "df.dep = df.dep.apply(lambda x: x.replace('2A', '20').replace('2B', '20'))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>...</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>971</th>\n",
       "      <th>972</th>\n",
       "      <th>973</th>\n",
       "      <th>974</th>\n",
       "      <th>976</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1993.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1993.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1993.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1993.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1993.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1  10  11  12  13  14  15  16  17  18   ...    92  93  94  95  971  972  \\\n",
       "0  1   0   0   0   0   0   0   0   0   0   ...     0   0   0   0    0    0   \n",
       "1  1   0   0   0   0   0   0   0   0   0   ...     0   0   0   0    0    0   \n",
       "2  1   0   0   0   0   0   0   0   0   0   ...     0   0   0   0    0    0   \n",
       "3  1   0   0   0   0   0   0   0   0   0   ...     0   0   0   0    0    0   \n",
       "4  0   0   0   0   0   0   0   0   0   0   ...     0   0   0   0    0    0   \n",
       "\n",
       "   973  974  976    year  \n",
       "0    0    0    0  1993.0  \n",
       "1    0    0    0  1993.0  \n",
       "2    0    0    0  1993.0  \n",
       "3    0    0    0  1993.0  \n",
       "4    0    0    0  1993.0  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Stockons dÃ¨s maintenant une binarisation des dÃ©partements\n",
    "#garder l'annÃ©e au cas oÃ¹\n",
    "\n",
    "ddeps = pd.get_dummies(df.dep).join(df.year)\n",
    "ddeps.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "groups = ['EG', 'G', 'C', 'D', 'ED', 'autre']\n",
    "lgroups = ['l_%s' % g for g in groups]\n",
    "pgroups = ['presid_%s' % g for g in groups]\n",
    "\n",
    "#on stocke le \"gagnant\" de chaque circonscription\n",
    "df['l_win'] = df[lgroups].idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#On test des classifications en utilisant juste les prÃ©sidentielles\n",
    "X = df[pgroups]\n",
    "y = df.l_win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 0.05565508,  0.03636726,  0.04080391,  0.04184739,  0.04290032,\n",
       "         0.04259205,  0.04279065]),\n",
       " 'mean_score_time': array([ 0.01329557,  0.00205533,  0.00203737,  0.00200868,  0.00204102,\n",
       "         0.00206168,  0.00201035]),\n",
       " 'mean_test_score': array([ 0.44676471,  0.42529412,  0.42647059,  0.425     ,  0.425     ,\n",
       "         0.425     ,  0.425     ]),\n",
       " 'mean_train_score': array([ 0.58124111,  0.58344953,  0.58344888,  0.58344888,  0.58344888,\n",
       "         0.58344888,  0.58344888]),\n",
       " 'param_C': masked_array(data = [0.001 0.01 0.1 1 10 100 1000],\n",
       "              mask = [False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'params': ({'C': 0.001},\n",
       "  {'C': 0.01},\n",
       "  {'C': 0.1},\n",
       "  {'C': 1},\n",
       "  {'C': 10},\n",
       "  {'C': 100},\n",
       "  {'C': 1000}),\n",
       " 'rank_test_score': array([1, 3, 2, 4, 4, 4, 4], dtype=int32),\n",
       " 'split0_test_score': array([ 0.27112676,  0.25616197,  0.25528169,  0.25528169,  0.25528169,\n",
       "         0.25528169,  0.25528169]),\n",
       " 'split0_train_score': array([ 0.68109541,  0.68727915,  0.68639576,  0.68639576,  0.68639576,\n",
       "         0.68639576,  0.68639576]),\n",
       " 'split1_test_score': array([ 0.43071492,  0.37775816,  0.37775816,  0.3733451 ,  0.3733451 ,\n",
       "         0.3733451 ,  0.3733451 ]),\n",
       " 'split1_train_score': array([ 0.5513895 ,  0.55227172,  0.55227172,  0.55227172,  0.55227172,\n",
       "         0.55227172,  0.55227172]),\n",
       " 'split2_test_score': array([ 0.63925729,  0.64279399,  0.64721485,  0.64721485,  0.64721485,\n",
       "         0.64721485,  0.64721485]),\n",
       " 'split2_train_score': array([ 0.51123843,  0.51079771,  0.51167915,  0.51167915,  0.51167915,\n",
       "         0.51167915,  0.51167915]),\n",
       " 'std_fit_time': array([ 0.03165473,  0.0006791 ,  0.00061788,  0.00113378,  0.00135129,\n",
       "         0.00150058,  0.00179408]),\n",
       " 'std_score_time': array([  1.50830176e-02,   6.92083137e-05,   5.13496773e-05,\n",
       "          5.48189938e-06,   2.32742888e-05,   6.03496824e-05,\n",
       "          2.59083960e-05]),\n",
       " 'std_test_score': array([ 0.15072702,  0.16139061,  0.1636811 ,  0.16413135,  0.16413135,\n",
       "         0.16413135,  0.16413135]),\n",
       " 'std_train_score': array([ 0.07248534,  0.07534572,  0.07465692,  0.07465692,  0.07465692,\n",
       "         0.07465692,  0.07465692])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gridsearch sur plusieurs hyperparamÃ¨tres. Pour rÃ©gression logistique, on test diffÃ©rentes forces de pÃ©nalisation\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000] }\n",
    "\n",
    "clf = GridSearchCV(LogisticRegression(), param_grid)\n",
    "\n",
    "clf.fit(X, y)\n",
    "\n",
    "clf.cv_results_\n",
    "\n",
    "#manifestement c'est la plus forte pÃ©nalisation (C=0.001) qui donne le meilleur std test score... 0.44 de prÃ©cision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Testons en limitant le nombre d'annees puisque l'apport des rÃ©sultats de 2017 dans le jeu d'entraÃ®nement n'amÃ©liore pas\n",
    "#les scores de validation\n",
    "\n",
    "X2 = df[df.year >= 2012.0][pgroups]\n",
    "y2 = df[df.year >= 2012.0].l_win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'penalty': 'l1', 'C': 0.01}\n",
      "0.670250896057\n",
      "__\n",
      "{'penalty': 'l2', 'C': 0.001}\n",
      "0.668458781362\n",
      "__\n",
      "{'penalty': 'l2', 'C': 0.005}\n",
      "0.666666666667\n",
      "__\n",
      "{'penalty': 'l2', 'C': 0.01}\n",
      "0.666666666667\n",
      "__\n",
      "{'penalty': 'l2', 'C': 0.0005}\n",
      "0.661290322581\n",
      "__\n",
      "{'penalty': 'l1', 'C': 0.005}\n",
      "0.660394265233\n",
      "__\n",
      "{'penalty': 'l2', 'C': 0.0001}\n",
      "0.655913978495\n",
      "__\n",
      "{'penalty': 'l1', 'C': 0.0005}\n",
      "0.631720430108\n",
      "__\n",
      "{'penalty': 'l1', 'C': 0.001}\n",
      "0.621863799283\n",
      "__\n",
      "{'penalty': 'l1', 'C': 0.0001}\n",
      "0.385304659498\n",
      "__\n"
     ]
    }
   ],
   "source": [
    "#On rÃ©pÃ¨te le grid search avec seulement 2012 et 2017 comme donnÃ©es.\n",
    "#Cette fois en testant aussi la pÃ©nalisation l1, et avec surtout des petits C puisqu'on dirait une forte pÃ©nalisation\n",
    "#est nÃ©cessaire.\n",
    "\n",
    "param_grid = {'penalty': ['l1', 'l2'], 'C': [0.0001, 0.0005, 0.001, 0.005, 0.01] }\n",
    "\n",
    "clf = GridSearchCV(LogisticRegression(), param_grid, cv=10)\n",
    "\n",
    "clf.fit(X2, y2)\n",
    "\n",
    "for i in np.argsort(clf.cv_results_['rank_test_score'])[:10]:\n",
    "    print clf.cv_results_['params'][i]\n",
    "    print clf.cv_results_['mean_test_score'][i]\n",
    "    print '__'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En limitant le nombre d'annÃ©es, on limite les diffÃ©rentes situations politiques, d'oÃ¹ les meilleurs scores. C'est sÃ»rement de l'overfit mais on pensera au moins Ã  supprimer les annÃ©es 93 et 97 de l'entraÃ®nement plus tard. Finalement ces deux annÃ©es n'ont vraiment rien Ã  voir avec le reste des Ã©lections lÃ©gislatives puisqu'elles ne voient pas d'Ã©lection prÃ©sidentielle se dÃ©rouler.\n",
    "\n",
    "Partons sur des tests similaires pour Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#RF a l'air d'exiger un min_samples_split un minimum Ã©levÃ© pour Ã©viter l'overfit ?\n",
    "#On test aussi le nombre d'arbres et le critÃ¨re de sÃ©paration.\n",
    "\n",
    "#param_grid = {'n_estimators': [10, 50, 100], 'min_samples_split': [100, 200, 300, 400, 500, 600, 700], 'criterion': ['gini', 'entropy']}\n",
    "#\n",
    "#clf = GridSearchCV(RandomForestClassifier(), param_grid, cv=10)\n",
    "#\n",
    "#clf.fit(X, y)\n",
    "#\n",
    "#for i in np.argsort(clf.cv_results_['rank_test_score'])[:10]:\n",
    "#    print clf.cv_results_['params'][i]\n",
    "#    print clf.cv_results_['mean_test_score'][i]\n",
    "#    print '__'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le gridsearch est long Ã  effectuer pour RF. Les meilleurs scores sont : \n",
    "\n",
    "{'min_samples_split': 600, 'n_estimators': 100, 'criterion': 'gini'}\n",
    "0.562647058824\n",
    "\n",
    "{'min_samples_split': 600, 'n_estimators': 50, 'criterion': 'gini'}\n",
    "0.557352941176\n",
    "\n",
    "{'min_samples_split': 600, 'n_estimators': 10, 'criterion': 'gini'}\n",
    "0.556764705882\n",
    "\n",
    "{'min_samples_split': 300, 'n_estimators': 50, 'criterion': 'gini'}\n",
    "0.556176470588\n",
    "\n",
    "{'min_samples_split': 500, 'n_estimators': 50, 'criterion': 'entropy'}\n",
    "0.555882352941\n",
    "\n",
    "{'min_samples_split': 300, 'n_estimators': 10, 'criterion': 'gini'}\n",
    "0.555882352941\n",
    "\n",
    "{'min_samples_split': 600, 'n_estimators': 100, 'criterion': 'entropy'}\n",
    "0.555588235294\n",
    "\n",
    "{'min_samples_split': 500, 'n_estimators': 10, 'criterion': 'gini'}\n",
    "0.555\n",
    "\n",
    "{'min_samples_split': 500, 'n_estimators': 50, 'criterion': 'gini'}\n",
    "0.554705882353\n",
    "\n",
    "{'min_samples_split': 700, 'n_estimators': 50, 'criterion': 'gini'}\n",
    "0.553235294118\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Retestons avec juste les donnÃ©es 2012 et 2017\n",
    "\n",
    "\n",
    "#param_grid = {'n_estimators': [10, 25, 50, 75, 100], 'min_samples_split': [150, 250, 350, 450], 'criterion': ['gini', 'entropy']}\n",
    "#\n",
    "#clf = GridSearchCV(RandomForestClassifier(), param_grid, cv=10)\n",
    "#\n",
    "#clf.fit(X2, y2)\n",
    "#\n",
    "##10 combinaisons aux meilleurs scores, et leurs scores\n",
    "#\n",
    "#for i in np.argsort(clf.cv_results_['rank_test_score'])[:10]:\n",
    "#    print clf.cv_results_['params'][i]\n",
    "#    print clf.cv_results_['mean_test_score'][i]\n",
    "#    print '__'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour 2012 et 2017 les meilleurs scores sont :\n",
    "{'min_samples_split': 250, 'n_estimators': 10, 'criterion': 'entropy'}\n",
    "0.6729390681\n",
    "\n",
    "{'min_samples_split': 250, 'n_estimators': 25, 'criterion': 'entropy'}\n",
    "0.664874551971\n",
    "\n",
    "{'min_samples_split': 250, 'n_estimators': 25, 'criterion': 'gini'}\n",
    "0.662186379928\n",
    "\n",
    "{'min_samples_split': 250, 'n_estimators': 50, 'criterion': 'entropy'}\n",
    "0.660394265233\n",
    "\n",
    "{'min_samples_split': 150, 'n_estimators': 50, 'criterion': 'entropy'}\n",
    "0.656810035842\n",
    "\n",
    "{'min_samples_split': 250, 'n_estimators': 75, 'criterion': 'gini'}\n",
    "0.656810035842\n",
    "\n",
    "{'min_samples_split': 250, 'n_estimators': 100, 'criterion': 'gini'}\n",
    "0.656810035842\n",
    "\n",
    "{'min_samples_split': 250, 'n_estimators': 75, 'criterion': 'entropy'}\n",
    "0.655913978495\n",
    "\n",
    "{'min_samples_split': 150, 'n_estimators': 100, 'criterion': 'gini'}\n",
    "0.655017921147\n",
    "\n",
    "{'min_samples_split': 250, 'n_estimators': 100, 'criterion': 'entropy'}\n",
    "0.655017921147\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Maintenant on s'intÃ©resse Ã  2007/2012\n",
    "\n",
    "\n",
    "X1 = df[(df.year == 2007) | (df.year == 2012)][pgroups]\n",
    "y1 = df[(df.year == 2007) | (df.year == 2012)].l_win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'penalty': 'l2', 'C': 0.0005}\n",
      "0.675819309123\n",
      "__\n",
      "{'penalty': 'l2', 'C': 0.005}\n",
      "0.674933569531\n",
      "__\n",
      "{'penalty': 'l2', 'C': 0.01}\n",
      "0.674933569531\n",
      "__\n",
      "{'penalty': 'l1', 'C': 0.005}\n",
      "0.674047829938\n",
      "__\n",
      "{'penalty': 'l2', 'C': 1}\n",
      "0.674047829938\n",
      "__\n",
      "{'penalty': 'l2', 'C': 0.001}\n",
      "0.673162090345\n",
      "__\n",
      "{'penalty': 'l2', 'C': 0.1}\n",
      "0.673162090345\n",
      "__\n",
      "{'penalty': 'l1', 'C': 1}\n",
      "0.673162090345\n",
      "__\n",
      "{'penalty': 'l1', 'C': 0.01}\n",
      "0.669619131975\n",
      "__\n",
      "{'penalty': 'l1', 'C': 0.1}\n",
      "0.668733392383\n",
      "__\n"
     ]
    }
   ],
   "source": [
    "#Pour lr\n",
    "\n",
    "param_grid = {'penalty': ['l1', 'l2'], 'C': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1, 1, 10] }\n",
    "\n",
    "clf = GridSearchCV(LogisticRegression(), param_grid, cv=10)\n",
    "\n",
    "clf.fit(X1, y1)\n",
    "\n",
    "for i in np.argsort(clf.cv_results_['rank_test_score'])[:10]:\n",
    "    print clf.cv_results_['params'][i]\n",
    "    print clf.cv_results_['mean_test_score'][i]\n",
    "    print '__'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Puisque scores meilleurs sur les plus gros min_samples_split, on se concentre dessus\n",
    "\n",
    "\n",
    "#param_grid = {'n_estimators': [10, 25, 50, 75, 100], 'min_samples_split': [450, 550, 650, 750], 'criterion': ['gini', 'entropy']}\n",
    "#\n",
    "#clf = GridSearchCV(RandomForestClassifier(), param_grid, cv=10)\n",
    "#\n",
    "#clf.fit(X1, y1)\n",
    "#\n",
    "##10 combinaisons aux meilleurs scores, et leurs scores\n",
    "#\n",
    "#for i in np.argsort(clf.cv_results_['rank_test_score'])[:10]:\n",
    "#    print clf.cv_results_['params'][i]\n",
    "#    print clf.cv_results_['mean_test_score'][i]\n",
    "#    print '__'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 meilleurs scores :\n",
    "\n",
    "{'min_samples_split': 450, 'n_estimators': 100, 'criterion': 'entropy'}\n",
    "0.651018600531\n",
    "\n",
    "{'min_samples_split': 450, 'n_estimators': 10, 'criterion': 'entropy'}\n",
    "0.647475642161\n",
    "\n",
    "{'min_samples_split': 450, 'n_estimators': 75, 'criterion': 'entropy'}\n",
    "0.646589902569\n",
    "\n",
    "{'min_samples_split': 450, 'n_estimators': 50, 'criterion': 'entropy'}\n",
    "0.645704162976\n",
    "\n",
    "{'min_samples_split': 450, 'n_estimators': 75, 'criterion': 'gini'}\n",
    "0.643932683791"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pour 2007/2012, seul :\n",
      "LR : 0.67642\n",
      "RF : 0.64100\n",
      "Pour 2007/2012, LR, dep rÃ©intÃ©grÃ©e : 0.67288\n",
      "Pour 2007/2012, RF, dep rÃ©intÃ©grÃ©e : 0.64457\n",
      "Pour 2007/2012, LR, circ rÃ©intÃ©grÃ©e : 0.67198\n",
      "Pour 2007/2012, RF, circ rÃ©intÃ©grÃ©e : 0.65524\n",
      "Pour 2007/2012, LR, year rÃ©intÃ©grÃ©e : 0.67642\n",
      "Pour 2007/2012, RF, year rÃ©intÃ©grÃ©e : 0.66244\n",
      "Pour 2007/2012, LR, pop rÃ©intÃ©grÃ©e : 0.66145\n",
      "Pour 2007/2012, RF, pop rÃ©intÃ©grÃ©e : 0.65347\n",
      "Pour 2007/2012, LR, dens rÃ©intÃ©grÃ©e : 0.66752\n",
      "Pour 2007/2012, RF, dens rÃ©intÃ©grÃ©e : 0.64633\n",
      "Pour 2007/2012, LR, age_1 rÃ©intÃ©grÃ©e : 0.67372\n",
      "Pour 2007/2012, RF, age_1 rÃ©intÃ©grÃ©e : 0.64902\n",
      "Pour 2007/2012, LR, age_2 rÃ©intÃ©grÃ©e : 0.67374\n",
      "Pour 2007/2012, RF, age_2 rÃ©intÃ©grÃ©e : 0.63652\n",
      "Pour 2007/2012, LR, age_3 rÃ©intÃ©grÃ©e : 0.67642\n",
      "Pour 2007/2012, RF, age_3 rÃ©intÃ©grÃ©e : 0.64902\n",
      "Pour 2007/2012, LR, age_4 rÃ©intÃ©grÃ©e : 0.67022\n",
      "Pour 2007/2012, RF, age_4 rÃ©intÃ©grÃ©e : 0.65080\n",
      "Pour 2007/2012, LR, etud rÃ©intÃ©grÃ©e : 0.67110\n",
      "Pour 2007/2012, RF, etud rÃ©intÃ©grÃ©e : 0.64010\n",
      "Pour 2007/2012, LR, hf rÃ©intÃ©grÃ©e : 0.67642\n",
      "Pour 2007/2012, RF, hf rÃ©intÃ©grÃ©e : 0.65344\n",
      "Pour 2007/2012, LR, chom rÃ©intÃ©grÃ©e : 0.67553\n",
      "Pour 2007/2012, RF, chom rÃ©intÃ©grÃ©e : 0.64546\n",
      "_______\n",
      "Pour 2012/2017, seul :\n",
      "LR : 0.67001\n",
      "RF : 0.65651\n",
      "Pour 2012/2017, LR, dep rÃ©intÃ©grÃ©e : 0.66194\n",
      "Pour 2012/2017, RF, dep rÃ©intÃ©grÃ©e : 0.66195\n",
      "Pour 2012/2017, LR, circ rÃ©intÃ©grÃ©e : 0.66729\n",
      "Pour 2012/2017, RF, circ rÃ©intÃ©grÃ©e : 0.66101\n",
      "Pour 2012/2017, LR, year rÃ©intÃ©grÃ©e : 0.66819\n",
      "Pour 2012/2017, RF, year rÃ©intÃ©grÃ©e : 0.66112\n",
      "Pour 2012/2017, LR, pop rÃ©intÃ©grÃ©e : 0.66104\n",
      "Pour 2012/2017, RF, pop rÃ©intÃ©grÃ©e : 0.65384\n",
      "Pour 2012/2017, LR, dens rÃ©intÃ©grÃ©e : 0.66197\n",
      "Pour 2012/2017, RF, dens rÃ©intÃ©grÃ©e : 0.66109\n",
      "Pour 2012/2017, LR, age_1 rÃ©intÃ©grÃ©e : 0.67182\n",
      "Pour 2012/2017, RF, age_1 rÃ©intÃ©grÃ©e : 0.65564\n",
      "Pour 2012/2017, LR, age_2 rÃ©intÃ©grÃ©e : 0.67001\n",
      "Pour 2012/2017, RF, age_2 rÃ©intÃ©grÃ©e : 0.65931\n",
      "Pour 2012/2017, LR, age_3 rÃ©intÃ©grÃ©e : 0.67001\n",
      "Pour 2012/2017, RF, age_3 rÃ©intÃ©grÃ©e : 0.66108\n",
      "Pour 2012/2017, LR, age_4 rÃ©intÃ©grÃ©e : 0.67001\n",
      "Pour 2012/2017, RF, age_4 rÃ©intÃ©grÃ©e : 0.65205\n",
      "Pour 2012/2017, LR, etud rÃ©intÃ©grÃ©e : 0.67001\n",
      "Pour 2012/2017, RF, etud rÃ©intÃ©grÃ©e : 0.65833\n",
      "Pour 2012/2017, LR, hf rÃ©intÃ©grÃ©e : 0.66643\n",
      "Pour 2012/2017, RF, hf rÃ©intÃ©grÃ©e : 0.65741\n",
      "Pour 2012/2017, LR, chom rÃ©intÃ©grÃ©e : 0.66819\n",
      "Pour 2012/2017, RF, chom rÃ©intÃ©grÃ©e : 0.66010\n"
     ]
    }
   ],
   "source": [
    "#Travail de rÃ©intÃ©gration des variables. Une variable supplÃ©mentaire\n",
    "\n",
    "#LR1 et RF1 correspondent aux meilleurs hyperparamÃ¨tres pour s'entraÃ®ner sur 2007 et 2012.\n",
    "#LR2 et RF2 pour s'entraÃ®ner sur 2012 et 2017.\n",
    "\n",
    "#Manifestement pour RF1 il faut ajouter age_3. Pour RF2 age_4.\n",
    "#Pour LR1 rien Ã  faire.\n",
    "\n",
    "LR1 = LogisticRegression(C=0.0005, penalty='l2')\n",
    "LR2 = LogisticRegression(C=0.01, penalty='l1')\n",
    "\n",
    "RF1 = RandomForestClassifier(min_samples_split=450, n_estimators=100, criterion='entropy')\n",
    "RF2 = RandomForestClassifier(min_samples_split=250, n_estimators=75, criterion='entropy')\n",
    "\n",
    "l1 = {}\n",
    "l2 = {}\n",
    "\n",
    "for col in df.columns[:-12]:\n",
    "    l1[col] = X1.join(df[col], how='inner')\n",
    "    l2[col] = X2.join(df[col], how='inner')\n",
    "\n",
    "print \"Pour 2007/2012, seul :\"\n",
    "print \"LR : %0.5f\" % np.mean(cross_val_score(LR1, X1, y1, cv=10))\n",
    "print \"RF : %0.5f\" % np.mean(cross_val_score(RF1, X1, y1, cv=10))\n",
    "for col in df.columns[:-12]:\n",
    "    print \"Pour 2007/2012, LR, %s rÃ©intÃ©grÃ©e : %0.5f\" % (col, np.mean(cross_val_score(LR1, l1[col], y1, cv=10)))\n",
    "    print \"Pour 2007/2012, RF, %s rÃ©intÃ©grÃ©e : %0.5f\" % (col, np.mean(cross_val_score(RF1, l1[col], y1, cv=10)))\n",
    "\n",
    "print '_______'\n",
    "print \"Pour 2012/2017, seul :\"\n",
    "print \"LR : %0.5f\" % np.mean(cross_val_score(LR2, X2, y2, cv=10))\n",
    "print \"RF : %0.5f\" % np.mean(cross_val_score(RF2, X2, y2, cv=10))\n",
    "for col in df.columns[:-12]:\n",
    "    print \"Pour 2012/2017, LR, %s rÃ©intÃ©grÃ©e : %0.5f\" % (col, np.mean(cross_val_score(LR2, l2[col], y2, cv=10)))\n",
    "    print \"Pour 2012/2017, RF, %s rÃ©intÃ©grÃ©e : %0.5f\" % (col, np.mean(cross_val_score(RF2, l2[col], y2, cv=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pour 2007/2012, seul : 0.676416\n",
      "Pour 2007/2012, pop, age_3 : 0.680966\n",
      "_______\n",
      "Pour 2012/2017, seul : 0.660228\n"
     ]
    }
   ],
   "source": [
    "#Et maintenant on teste des combinaisons en ajoutant 2 variables Ã  la fois. Un algo Ã  la fois. LR\n",
    "\n",
    "ll1 = {}\n",
    "ll2 = {}\n",
    "\n",
    "for col1 in df.columns[:-12]:\n",
    "    for col2 in df.columns[:-12]:\n",
    "        if (col1 != col2) and ((col2, col1) not in ll1.keys()):\n",
    "            ll1[(col1, col2)] = X1.join(df[col1]).join(df[col2])\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "for col1 in df.columns[:-12]:\n",
    "    for col2 in df.columns[:-12]:\n",
    "        if (col1 != col2) and ((col2, col1) not in ll2.keys()):\n",
    "            ll2[(col1, col2)] = X2.join(df[col1]).join(df[col2])\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "print \"Pour 2007/2012, seul : %0.6f\" % np.mean(cross_val_score(LR1, X1, y1, cv=10))\n",
    "for (col1, col2) in ll1.keys():\n",
    "    score = np.mean(cross_val_score(LR1, ll1[col1, col2], y1, cv=10))\n",
    "    if score > 0.678:\n",
    "        print \"Pour 2007/2012, %s, %s : %0.6f\" % (col1, col2, score)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "print '_______'\n",
    "print \"Pour 2012/2017, seul : %0.6f\" % np.mean(cross_val_score(LR2, X2, y2))\n",
    "for (col1, col2) in ll2.keys():\n",
    "    score = np.mean(cross_val_score(LR2, ll2[col1, col2], y2))\n",
    "    if score > 0.678:\n",
    "        print \"Pour 2012/2017, %s, %s : %0.6f\" % (col1, col2, score)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65472915242652086"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pour RF1, ajouter age_2 chom donne 64.46%\n",
    "#Pour RF2, pop et hf (!) donne 0.6665 . Il y a aussi age3 etud qui donne 0.6655. Bref on remonte trÃ¨s lÃ©gÃ¨rement au\n",
    "#dessus du score de base.\n",
    "\n",
    "\n",
    "#print \"Pour 2007/2012, seul : %0.6f\" % np.mean(cross_val_score(RF1, X1, y1, cv=10))\n",
    "#for (col1, col2) in ll1.keys():\n",
    "#    score = np.mean(cross_val_score(RF1, ll1[col1, col2], y1, cv=10))\n",
    "#    if score > 0.65:\n",
    "#        print \"Pour 2007/2012, %s, %s : %0.6f\" % (col1, col2, score)\n",
    "#    else:\n",
    "#        pass\n",
    "\n",
    "#print '_______'\n",
    "#print \"Pour 2012/2017, seul : %0.6f\" % np.mean(cross_val_score(RF2, X2, y2, cv=10))\n",
    "#for (col1, col2) in ll2.keys():\n",
    "#    score = np.mean(cross_val_score(RF2, ll2[col1, col2], y2, cv=10))\n",
    "#    if score > 0.658:\n",
    "#        print \"Pour 2012/2017, %s, %s : %0.6f\" % (col1, col2, score)\n",
    "#    else:\n",
    "#        pass\n",
    "\n",
    "np.mean(cross_val_score(RF2, X2, y2, cv=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Enfin, on teste la sÃ©lection de features avec selectfrommodel.\n",
    "\n",
    "#On met en place des frames qui contiennent toutes les variables de train.\n",
    "xcols = df.columns\n",
    "for lg in lgroups:\n",
    "    xcols.remove(lg)\n",
    "xcols.remove('l_win')\n",
    "Xtot = df[xcols]\n",
    "X1tot = df[(df.year == 2007) | (df.year == 2012)][xcols]\n",
    "X2tot = df[(df.year == 2012) | (df.year == 2017)][xcols]\n",
    "\n",
    "\n",
    "PipeLR1 = Pipeline([('feature_selection', SelectFromModel(LR1)),\n",
    "                   ('clf', LR1)])\n",
    "PipeLR2 = Pipeline([('feature_selection', SelectFromModel(LR2)),\n",
    "                   ('clf', LR2)])\n",
    "\n",
    "PipeRF1 = Pipeline([('feature_selection', SelectFromModel(RF1)),\n",
    "                   ('clf', RF1)])\n",
    "PipeRF2 = Pipeline([('feature_selection', SelectFromModel(RF2)),\n",
    "                   ('clf', RF2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2007/2012, LR : 0.67734\n",
      "2007/2012, RF : 0.64993\n",
      "__\n",
      "2012/2017, LR : 0.66018\n",
      "2012/2017, RF : 0.66556\n"
     ]
    }
   ],
   "source": [
    "print \"2007/2012, LR : %0.5f\" % np.mean(cross_val_score(PipeLR1, X1tot, y1, cv=10))\n",
    "print \"2007/2012, RF : %0.5f\" % np.mean(cross_val_score(PipeRF1, X1tot, y1, cv=10))\n",
    "print '__'\n",
    "print \"2012/2017, LR : %0.5f\" % np.mean(cross_val_score(PipeLR2, X2tot, y2, cv=10))\n",
    "print \"2012/2017, RF : %0.5f\" % np.mean(cross_val_score(PipeRF2, X2tot, y2, cv=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.676415680434\n",
      "0.670005126452\n",
      "0.649049056382\n",
      "0.65833304853\n"
     ]
    }
   ],
   "source": [
    "#Pour mÃ©moire\n",
    "print np.mean(cross_val_score(LR1, X1, y1, cv=10))\n",
    "print np.mean(cross_val_score(LR2, X2, y2, cv=10))\n",
    "print np.mean(cross_val_score(RF1, X1, y1, cv=10))\n",
    "print np.mean(cross_val_score(RF2, X2, y2, cv=10))\n",
    "\n",
    "#Donc les scores n'Ã©voluent pas beaucoup. Peut-Ãªtre qu'il aurait mieux valu sÃ©lectionner les features avec un autre\n",
    "#modÃ¨le que celui qu'on utilise pour les prÃ©dictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Juste histoire de voir les variables choisies par les sfm :\n",
    "\n",
    "sfm_LR1 = SelectFromModel(LR1)\n",
    "sfm_LR2 = SelectFromModel(LR2)\n",
    "\n",
    "sfm_RF1 = SelectFromModel(RF1)\n",
    "sfm_RF2 = SelectFromModel(RF2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sfm_LR1.fit(X1tot, y1)\n",
    "Xsfm_LR1 = sfm_LR1.transform(X1tot)\n",
    "\n",
    "sfm_LR2.fit(X2tot, y2)\n",
    "Xsfm_LR2 = sfm_LR2.transform(X2tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 19.  ,  19.62,  34.5 ,  15.97,   9.24,  20.67],\n",
       "       [ 19.  ,  19.62,  34.5 ,  15.97,   9.24,  20.67],\n",
       "       [ 19.  ,  19.62,  34.5 ,  15.97,   9.24,  20.67],\n",
       "       ..., \n",
       "       [ 20.  ,  11.24,  34.24,  14.17,  12.76,  27.32],\n",
       "       [ 20.  ,  11.24,  34.24,  14.17,  12.76,  27.32],\n",
       "       [ 20.  ,  11.24,  34.24,  14.17,  12.76,  27.32]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ici on a juste exclu presid_autre et intÃ©grÃ© age_4\n",
    "Xsfm_LR1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.00000000e+00,   2.01200000e+03,   1.06000000e+02, ...,\n",
       "          2.29500000e+01,   1.34600000e+01,   2.27500000e+01],\n",
       "       [  1.00000000e+00,   2.01200000e+03,   1.06000000e+02, ...,\n",
       "          2.29500000e+01,   1.34600000e+01,   2.27500000e+01],\n",
       "       [  1.00000000e+00,   2.01200000e+03,   1.06000000e+02, ...,\n",
       "          2.29500000e+01,   1.34600000e+01,   2.27500000e+01],\n",
       "       ..., \n",
       "       [  9.74000000e+02,   2.01700000e+03,   3.39000000e+02, ...,\n",
       "          2.63500000e+01,   2.72600000e+01,   7.67000000e+00],\n",
       "       [  9.76000000e+02,   2.01700000e+03,   6.25000000e+02, ...,\n",
       "          3.02700000e+01,   1.13000000e+01,   4.33000000e+00],\n",
       "       [  9.76000000e+02,   2.01700000e+03,   6.25000000e+02, ...,\n",
       "          3.02700000e+01,   1.13000000e+01,   4.33000000e+00]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ici par contre on est sur du dep, year, dens, ED, EG, G\n",
    "Xsfm_LR2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sfm_RF1.fit(X1tot, y1)\n",
    "Xsfm_RF1 = sfm_RF1.transform(X1tot)\n",
    "\n",
    "sfm_RF2.fit(X2tot, y2)\n",
    "Xsfm_RF2 = sfm_RF2.transform(X2tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.00700000e+03,   1.96200000e+01,   3.45000000e+01,\n",
       "          9.24000000e+00,   2.06700000e+01,   0.00000000e+00],\n",
       "       [  2.00700000e+03,   1.96200000e+01,   3.45000000e+01,\n",
       "          9.24000000e+00,   2.06700000e+01,   0.00000000e+00],\n",
       "       [  2.00700000e+03,   1.96200000e+01,   3.45000000e+01,\n",
       "          9.24000000e+00,   2.06700000e+01,   0.00000000e+00],\n",
       "       ..., \n",
       "       [  2.01200000e+03,   1.12400000e+01,   3.42400000e+01,\n",
       "          1.27600000e+01,   2.73200000e+01,   2.80000000e-01],\n",
       "       [  2.01200000e+03,   1.12400000e+01,   3.42400000e+01,\n",
       "          1.27600000e+01,   2.73200000e+01,   2.80000000e-01],\n",
       "       [  2.01200000e+03,   1.12400000e+01,   3.42400000e+01,\n",
       "          1.27600000e+01,   2.73200000e+01,   2.80000000e-01]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#year, C, D, EG, G, autre\n",
    "Xsfm_RF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.01200000e+03,   1.01600000e+01,   3.04100000e+01,\n",
       "          1.34600000e+01,   2.27500000e+01,   2.70000000e-01],\n",
       "       [  2.01200000e+03,   1.01600000e+01,   3.04100000e+01,\n",
       "          1.34600000e+01,   2.27500000e+01,   2.70000000e-01],\n",
       "       [  2.01200000e+03,   1.01600000e+01,   3.04100000e+01,\n",
       "          1.34600000e+01,   2.27500000e+01,   2.70000000e-01],\n",
       "       ..., \n",
       "       [  2.01700000e+03,   1.89100000e+01,   1.72600000e+01,\n",
       "          2.72600000e+01,   7.67000000e+00,   2.54000000e+00],\n",
       "       [  2.01700000e+03,   1.92100000e+01,   3.26200000e+01,\n",
       "          1.13000000e+01,   4.33000000e+00,   2.27000000e+00],\n",
       "       [  2.01700000e+03,   1.92100000e+01,   3.26200000e+01,\n",
       "          1.13000000e+01,   4.33000000e+00,   2.27000000e+00]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#year, C, D, EG, G, autre\n",
    "Xsfm_RF2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La sÃ©lection automatique de features n'augmente pas beaucoup les scores de validation. Mais elle nous indique qu'on pourrait supprimer certains des scores de prÃ©sidentielles des variables d'entraÃ®nement, puisqu'ils sont dÃ©pendants les uns des autres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#quelle prediction de 2017 pour LR1 et RF1 ?\n",
    "\n",
    "LR1.fit(X1, y1)\n",
    "RF1.fit(X1, y1)\n",
    "\n",
    "test = df[df.year == 2017][pgroups]\n",
    "\n",
    "p_lr1 = LR1.predict(test)\n",
    "p_rf1 = RF1.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'l_D': 57, 'l_G': 501})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(p_lr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'l_D': 239, 'l_G': 319})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(p_rf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'l_D': 171, 'l_G': 387})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pas terrible comme prÃ©diction, surtout quand on voit ce qu'il est advenu des candidats PS cette annÃ©e.\n",
    "#Mieux avec quelques variables rÃ©intÃ©grÃ©es ? J'en doute...\n",
    "\n",
    "#Pour LR1 le meilleur score obtenu Ã©tait 68% en rÃ©intÃ©grant pop et age_3\n",
    "X1b = X1.join(df[['pop', 'age_3']])\n",
    "LR1.fit(X1b, y1)\n",
    "\n",
    "test2 = test.join(df[['pop', 'age_3']])\n",
    "\n",
    "p_lr1 = LR1.predict(test2)\n",
    "Counter(p_lr1)\n",
    "#Pas beaucoup mieux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'l_D': 249, 'l_G': 309})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pour RF1 le meilleur score obtenu Ã©tait 66.2% en rÃ©intÃ©grant year... Peu probable que l'annÃ©e ait une valeur\n",
    "#explicative et le modÃ¨le va certainement s'en servir pour faire le tri entre l'annÃ©e 2007 et l'annÃ©e 2012.\n",
    "X1c = X1.join(df['year'])\n",
    "RF1.fit(X1c, y1)\n",
    "\n",
    "test3 = test.join(df['year'])\n",
    "\n",
    "p_rf1 = RF1.predict(test3)\n",
    "Counter(p_rf1)\n",
    "#On est toujours sur une prÃ©diction binaire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Le score de prÃ©cision semble Ãªtre un indicateur insuffisant de la pertinence des modÃ¨les. MÃªme en jouant avec les variables d'entrÃ©e, les annÃ©es utilisÃ©es pour l'entraÃ®nement et les hyperparamÃ¨tres pour amÃ©liorer le score, on reste sur une prÃ©diction qui donne l'avantage Ã  la gauche et Ã  la droite. Normal, c'est historiquement les deux groupes qui ont \"remportÃ©\" le plus de circonscriptions. Peut-Ãªtre qu'on devrait faire quelque chose pour que l'algorithme ne donne pas un avantage si prononcÃ© Ã  ces deux groupes ? On pourrait par exemple cloner des lignes des autres classes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
